---
title: "Data Science in the Organization: Things I Learned in 2016"
author: "Thomas Speidel"
date: 2016-12-30T21:13:14-05:00
categories: ["R"]
tags: ["rstats", "datascience"]
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(collapse = TRUE)
```

#useR! 2016
![](stanford.jpg)
The [useR! conference](http://user2016.org/) in Stanford was awesome. Never did I expect to see so many participants (read: data geeks) and high level [sponsors](http://user2016.org/#sponsors). For what was a little known language that's been around in one form or another since the late seventies, this is quite amazing. Some memorable talks:

*    People are doing really amazing things with **Shiny**. I was amazed at the pervasiveness of Shiny at [EA Games](http://channel9.msdn.com/Events/useR-international-R-User-conference/useR2016/RServer-Operationalizing-R-at-Electronic-Arts).

*    R's superhero **Hadley Whickham**, never takes a break. His idea is to create a "[pit of success](http://channel9.msdn.com/Events/useR-international-R-User-conference/useR2016/Towards-a-grammar-of-interactive-graphics)" by having a uniform approach to import, tidy, transform, visualize, model and communicate.

*    Daniela Witten methodology on **[interpretable regression using convex penalties](http://channel9.msdn.com/Events/useR-international-R-User-conference/useR2016/Flexible-and-Interpretable-Regression-Using-Convex-Penalties)**: as I see it, it's an attempt to do the least damage when people want simple interpretation of non-linear coefficients.

*    Tal Galili gave a great presentation on **[heatmaps](http://channel9.msdn.com/Events/useR-international-R-User-conference/useR2016/Heatmaps-in-R-Overview-and-best-practices)** in R

*    Rick Beckett on **[40 years of S](http://channel9.msdn.com/Events/useR-international-R-User-conference/useR2016/Forty-years-of-S)** was fascinating: now I understand why the assignment operator in R is `<-` (hint: jump to 0:29:00). We also learned that S and UNIX were two Bell Labs projects that grew together. Ironic how both took a while to be picked up and both grew into massively popular platforms (Linux, R).

<br><br>

# Data-Driven Decisions

As a long-time data practitioner, I don't get the cult-like infatuation with the term "data-driven" decisions (see here, here or here). Really folks: in the best case scenario we go from: data to evidence to decisions. If we want to go from data to decisions, hope for the best, because science may not back you up.

In health research, the term "data driven" has negative connotations. Instead, we have a much more scientifically sound **[evidence-based]*(http://en.wikipedia.org/wiki/Evidence-based_medicine)* approach.

![](evidence_based.png)

A recent study suggests that [women make better doctors than men](http://jamanetwork.com/journals/jamainternalmedicine/article-abstract/2593252) (also [here](http://www.theatlantic.com/health/archive/2016/12/female-doctors-superiority/511034/) and [here](http://www.nbcnews.com/health/health-news/female-doctors-outperform-male-doctors-according-study-n697876)). There are [many problems](http://stream.org/whos-better-playing-doctor-boys-girls/) with this study. But it got me thinking of a way to explain how "data driven" can be dangerous: if women are better doctor because they treated many fewer patients than men and, therefore, had more time to spend with patients, then, the data-driven decision of having more women doctors won't lead to improved health outcomes. But, hey, it's "data driven"!

This echos well-known statistician [Frank Harrell](http://vkc.mc.vanderbilt.edu/people/harrell-frank):

>    Using the data to guide the data analysis is almost as dangerous as not doing so".


<br><br>


# Ethics, Ethics, Ethics
![](ethics.jpg)

The ties between **statistics and ethics** are well known (see [here](http://www.jstor.org/stable/3564481?seq=1#page_scan_tab_contents)). Nearly every PhD biostatistician I know has sat on an ethics board at some points in their careers. Now, it seems that the broader field of **Data Science is starting to pay attention to the ethical implications of their work**. A few key events:

*    A great article on ProPublica on [predictive sentencing](http://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing) in the US.
*    More and more [courses](http://www.edx.org/course/data-science-ethics-michiganx-ds101x-1) on ethics in data science are starting to emerge
*    Cathy O'Neal's book "[Weapon of Math Destruction: How Big Data Increases Inequality and Threatens Democracy](http://blogs.scientificamerican.com/roots-of-unity/review-weapons-of-math-destruction/)" is on my [Amazon wish list](http://a.co/0weW8WA).
*    Google provides one of the [best explanations](http://research.google.com/bigpicture/attacking-discrimination-in-ml/) I have seen on the difficulty of making fair decisions. As [Thomas Lumley](http://www.statschat.org.nz/2017/01/01/kinds-of-fairness-worth-working-for/) writes: "**You have to decide what summary of the difference you care about, because you canâ€™t make them all the same**. This is old news in medical diagnostics, but appears not to have been considered in some other areas".

<br><br>

#Unicorns
![](https://s-media-cache-ak0.pinimg.com/736x/9d/0e/e6/9d0ee6c0049e088172590e0d81b1684c--polygon-art-diy-paper.jpg)
This year has confirmed that Data Science is a **wide and broad field**, just like Medicine, Law or Engineering: many flavours, many specializations. Sorry folks, **unicorns who are specialist in everything do not exist**.

Just like hospitals do not hire a family physicians and expect them to do the work of 100 specialists, just like we don't hire a criminal lawyer when we need a tax lawyer, organizations cannot expect to hire one data scientist generalist and do the work of 100 specialists. [**Diversity**](http://drewconway.com/zia/2013/7/18/warning-do-not-feed-the-wildebeests) is key.

And let's not fall into the trap of judging a Data Scientist by the **mathematical or algorithmic sophistication** of their arsenals. In the words of [Kahneman](http://en.wikipedia.org/wiki/Daniel_Kahneman), we would be substituting a question which we cannot answer with a different one which we can answer.

<br><br>


#Automation & Model Factories
![](automation.png)
Automation is really misunderstood. Really. I was pointed to **Tom Davenport's** article on "[Autonomous Analytics](http://www.linkedin.com/pulse/move-your-analytics-operation-from-artisanal-tom-davenport?trk=hp-feed-article-title-like)" and the concept of **model factory** as described in [this video](http://www.youtube.com/watch?v=yNfsnv9gjrU). It's a great article and agree with pretty much everything. It's what it **does not say** that concerns me. I commented in Tom's article, but briefly:

*    Not everything is worth automating (e.g. single points decisions)
*    Not everything is automatable (causal models, explanatory models, exploratory analysis)
*    Automation is more of an IT problem than a Data Science one (see **Diego Kuonen's** fantastic slides on [Demystifying Big Data, Data Science and Statistics](http://www.slideshare.net/kuonen/demystifying-big-data-data-science-and-statistics-along-with-machine-intelligence-and-learning)). *Which means it is the last mile of a repetitive insight generation process the mechanisms of which are well understood and that applies to a well understood class of problems (predictive)*.


<br><br>


#Data Wrangling
![](data_wrangling.jpg)

[Data munging/carpentry/wrangling](http://www.nytimes.com/2014/08/18/technology/for-big-data-scientists-hurdle-to-insights-is-janitor-work.html?_r=0), you know, that tedious, time consuming work that **takes some 80% of a data scientist's time** is (still) not given enough credit by evangelists, vendors, consultants and speakers. Not sexy? Sure, but it's a reality we must face nonetheless.

What's more, is hardly automatable (see above) and the [human-in-the-loop](http://www.oreilly.com/ideas/why-data-preparation-frameworks-rely-on-human-in-the-loop-systems) provides myriads of necessary context and judgement calls that machines cannot make. I bet this has caused a lot of misunderstanding in organizations: "*What? Are you trying to tell me we spent $100K for that fancy analytics software and it's useless?*"

<br><br>



#Analytical Maturity
![](http://xolotl.org/wp-content/uploads/2016/11/7159036564_ec69766fe7_o.png)

If the keywords and verbiage used by organizations to advertise their data related jobs are any indication of their **analytical maturity**, there are promising signs! I work in the energy sector, not Silicon Valley, which means it takes a little while for us to pick up on technological or scientific innovations. For the past four years, I received weekly job alerts and, anecdotally, I can say that the expectations, clarity and keywords of the ad have improved significantly.

<br><br>



#Analytics Podcasts
![](https://media.licdn.com/mpr/mpr/AAEAAQAAAAAAAAh-AAAAJGMxOWY3NzViLWEwYmItNDBkOS1iNDc3LTkyYTA5OGFmYjA4ZQ.png)

The number of really good analytics podcasts has grown significantly, to the point I can barely keep up.

*    [Not So Standard Deviation](http://soundcloud.com/nssd-podcast)
*    [The Data Skeptic Podcast](http://dataskeptic.com/)
*    [Partially Derivative](http://partiallyderivative.com/)
*    [Data Stories](http://datastori.es/)
*    [O'Reilly Data Show](http://www.oreilly.com/topics/oreilly-data-show-podcast)
*    [Talking Machines](http://www.thetalkingmachines.com/)

Not So Standard Deviations and O'Reilly Data Show are great detox if you are subject to a considerable amount of timeshare-sque-like products and hyperbolic claims by vendors or consultants.