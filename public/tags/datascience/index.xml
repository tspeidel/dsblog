<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Datascience on Alternative Stats</title>
    <link>https://alternative-stats.netlify.com/tags/datascience.html</link>
    <description>Recent content in Datascience on Alternative Stats</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2017 Thomas Speidel</copyright>
    <lastBuildDate>Sat, 30 Dec 2017 00:00:00 +0000</lastBuildDate>
    <atom:link href="/tags/datascience.html" rel="self" type="application/rss+xml" />
    
    <item>
      <title>What You Aren’t Told About Data Science</title>
      <link>https://alternative-stats.netlify.com/post/what-you-aren-t-told-about-data-science.html</link>
      <pubDate>Sat, 30 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>https://alternative-stats.netlify.com/post/what-you-aren-t-told-about-data-science.html</guid>
      <description>&lt;p&gt;&lt;img src=&#34;scrubbing.jpg&#34; height=&#34;200px&#34; width=&#34;700px&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Some time ago I started writing a post on data preparation which I never completed and eventually forgot. A recent LinkedIn post by Kevin Gray stimulated a rich conversation around: “&lt;a href=&#34;https://www.linkedin.com/feed/update/urn:li:activity:6352086326745096192&#34;&gt;Can Data Cleaning be automated?&lt;/a&gt;”. It reminded and enticed me to complete the post.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;div id=&#34;data-cleaning-and-data-preparation&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Data Cleaning and Data Preparation&lt;/h1&gt;
&lt;p&gt;When practitioners talk about data cleaning, they usually refer to a collection of tasks needed to &lt;strong&gt;make the data amenable for analysis&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Data is not necessarily invalid when we say they need to be cleaned. A common term in statistics that hasn’t fully caught-on in other areas is that of &lt;strong&gt;convenience data&lt;/strong&gt;: data that has been collected for reasons other than the purpose at task. Convenience data includes most of today’s “Big Data” and certainly most data collected for administrative purposes. Convenience data almost always require varying amounts of preparation because &lt;strong&gt;key variables for the task at hand may be missing&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;In 2014 on the height of the “Big-Data” hype, &lt;a href=&#34;https://www.nytimes.com/2014/08/18/technology/for-big-data-scientists-hurdle-to-insights-is-janitor-work.html&#34;&gt;Steve Lohr&lt;/a&gt; from the New York Times highlighted what practitioners have known for long time, but that came as a surprise to many. Lohr writes:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Working data scientists spend most of their time preparing data for analysis, a process that includes data collection, assessment, and transformation. Building an analysis data set is the first “hands-on” step in predictive analytics; analysts understand that this task is essntial for effective model building, and they invest as much time as needed to get it right.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Lohr goes on to highlight software tools capable of “finding, cleaning and blending data so that it is ready to be analyzed”. Most practitioners are highly skeptical of such claims; and for good reasons: they fail to recognize the complexity of the task. Unsurprisingly, it appears none of these software tools have caught up with data scientists. But why?&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;judgment-and-foresight&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Judgment and Foresight&lt;/h1&gt;
&lt;p&gt;Kuhn and Johnson shed some light in their popular book “&lt;a href=&#34;https://www.amazon.com/Applied-Predictive-Modeling-Max-Kuhn/dp/1461468485&#34;&gt;Applied Predictive Modeling (2013)&lt;/a&gt;”. They write:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;One of the first steps in the model building process is to transform, or encode, the original data structure into a form that is most informative for the model. This encoding process is critical and must be done with &lt;strong&gt;foresight into the analysis&lt;/strong&gt; that will be performed so that appropriate predictors can be elucidated from the original data.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;“&lt;strong&gt;Foresight into the analysis&lt;/strong&gt;” is the key sentence. Practitioners typically have a reasonable idea of what they are trying to achieve and what they do with the data is a reflection of that purpose. Let me illustrate how that “foresight” affects the choices we make with the data:&lt;/p&gt;
&lt;table class=&#34;table table-striped table-hover table-condensed&#34; style=&#34;float: left; margin-right: 10px;&#34;&gt;
&lt;caption&gt;
&lt;span id=&#34;tab:unnamed-chunk-1&#34;&gt;Table 1: &lt;/span&gt;&lt;br&gt; A fictitious customer purchase record: multiple rows per id
&lt;/caption&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Customer ID
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Purchase
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Item
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2015-06-15
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
A
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2013-08-25
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
C
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2013-08-26
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
A
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2013-08-05
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
D
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class=&#34;table table-striped table-hover table-condensed&#34; style=&#34;float: left; margin-right: 10px;&#34;&gt;
&lt;caption&gt;
&lt;span id=&#34;tab:unnamed-chunk-1&#34;&gt;Table 1: &lt;/span&gt;&lt;br&gt; A fictitious customer purchase record: single row per id
&lt;/caption&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Customer ID
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Purchase Month
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
A Items
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
B Items
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
C Items
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
D Items
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
June
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
August
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
August
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class=&#34;table table-striped table-hover table-condensed&#34; style=&#34;float: left; margin-right: 10px;&#34;&gt;
&lt;caption&gt;
&lt;span id=&#34;tab:unnamed-chunk-1&#34;&gt;Table 1: &lt;/span&gt;&lt;br&gt; A fictitious customer purchase record: multiple row per id and one row for each day from earliest purchase till today
&lt;/caption&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Customer ID
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Purchase
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Item
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2015-06-15
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
A
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2015-06-16
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2015-06-17
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
…
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
…
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2017-12-30
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2013-08-25
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
C
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2013-08-26
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
A
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2013-08-27
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
…
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
…
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2017-12-30
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2013-08-05
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
D
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2013-08-06
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
…
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
…
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2017-12-30
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;All three data-sets are realizations of the same underlying data, a fictitious customer purchase record. Neither is right or wrong, yet they all represent different “foresight” into how we attempt to analyze the data. There could be hundreds of ways to represent these data and each one of them may address a different analytically objective.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;can-data-be-clean&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Can Data Be Clean?&lt;/h1&gt;
&lt;p&gt;So, what’s clean data? Notwithstanding data entry errors, it may not exist writes &lt;a href=&#34;http://www.mimno.org/articles/carpentry/&#34;&gt;David Mimno (2014)&lt;/a&gt; from Cornell University:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;To me these imply that there is some kind of pure or clean data buried in a thin layer of non-clean data, and that one need only hose the dataset of to reveal the hard porcelain underneath the muck. In reality, the process is more like deciding how to cut into a piece of material, or how much to plane down a surface. It’s not that there’s any real distinction between good and bad, it’s more that some parts are or knottier than others. &lt;strong&gt;Judgement is critical&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;Foresight and now judgement&lt;/strong&gt;: two very human traits, both hard to automate except in very narrow problems.&lt;/p&gt;
&lt;p&gt;We must also remind ourselves that clean data may not be achievable in the first place. With respect to data with errors, Bill Winkler provides plenty of examples on the complexities of &lt;a href=&#34;https://en.wikipedia.org/wiki/Record_linkage&#34;&gt;record linkage&lt;/a&gt; where exact matches may not be feasible to achieve (see &lt;a href=&#34;http://onlinelibrary.wiley.com/doi/10.1002/wics.1317/abstract&#34;&gt;Winkler, 2014&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.amazon.com/Advanced-Analytics-Methodologies-Driving-Business/dp/0133498603&#34;&gt;Chambers and Dinsmore (2014)&lt;/a&gt; write on the importance of this time consuming steps:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Working data scientists spend most of their time preparing data for analytical a process that includes data collection, assessment, and transformation. Building an analysis data set is the first (hands-on&amp;quot; step in predictive analytics; analysts understand that this task is &lt;strong&gt;essential for effective model building&lt;/strong&gt;, and they invest as much time as needed to get it right.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;why-the-surprise&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Why the Surprise?&lt;/h1&gt;
&lt;p&gt;For practitioners, it is hardly a surprise data preparation is tedious and time consuming requiring foresight and judgement. Why the surprise then? &lt;a href=&#34;https://cs.uwaterloo.ca/~ilyas/&#34;&gt;Ihab Ilyas&lt;/a&gt; a professor at the University of Waterloo explains In a podcast interview on &lt;a href=&#34;https://www.oreilly.com/ideas/why-data-preparation-frameworks-rely-on-human-in-the-loop-systems&#34;&gt;O’Reilly Data Show&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;It has been also difficult to communicate these results to industry. And database practitioners, if you like, they were more into the well structured data and assuming a lot of good properties around this data, [and they were also] more interested in indexing this data, storing it, moving it from one place to another. And now, dealing with this large amount of diverse heterogeneous data with tons of errors, sidled across all business units in the same enterprise became a necessity. You cannot really avoid that anymore.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Perhaps the problem lies in part with analytically literacy: organizations have traditionally dealt with straight forward analytically needs for which the data at hand was sufficiently usable: &lt;strong&gt;filtering, joining, aggregating, adding, sorting&lt;/strong&gt; etc. For instance: &lt;em&gt;how many widgets were sold in store 11 in Q1 of 2016?&lt;/em&gt; In a typical organization, many inconsistencies of data were carried forward, at times unnoticed, and left to the consumer of the data to deal with. This paradigm no longer holds in the era of data as an asset where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the inconsistencies in the data may preclude more sophisticated analysis and certainly inferential and predictive objectives&lt;/li&gt;
&lt;li&gt;the data scientist both prepares and analyzes the data&lt;/li&gt;
&lt;li&gt;the data scientist expects to work with raw, unprocessed data&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Both Ilyas and Mimno highlight one aspect of doing data analysis data scientists may struggle with: &lt;strong&gt;single source of the truth&lt;/strong&gt;. This concept helps IT design robust data warehouse systems, but is a misleading principle to adhere to when doing analysis.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;can-data-preparation-be-automated&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Can Data Preparation Be Automated?&lt;/h1&gt;
&lt;p&gt;In my view, the short answer is no. Attempts to automate this tedious and time consuming task have largely failed because we do not know how to address the underlying problem. This does not mean that attempts at &lt;em&gt;facilitating&lt;/em&gt; data preparation tasks have failed. On the contrary, there has been progress in this area. In a &lt;a href=&#34;file:///home/tspeidel/GoogleDrive/dsblog/GitHub/dsblog/content/post/2017-12-30-what-you-aren-t-told-about-data-science.html&#34;&gt;recent Data Skeptic podcast&lt;/a&gt;, Microsoft’s Joseph Sirosh reports that:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Building on the latest research in program synthesis (PROSE) and data cleaning, Microsoft created a data wrangling technology that can drastically reduce the time that data scientists have to spend in coding and transforming data for machine learning. The way program synthesis works are, you give an input, the kind of data you want to wrangle, and the output you want to transform to, so you control the before and after.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I have not looked into these tools. Nonetheless, this seems significantly more tractable than auto-magically “&lt;em&gt;finding, cleaning and blending data so that it is ready to be analyzed&lt;/em&gt;”. A search in the academic literature reveals this to be an &lt;a href=&#34;https://scholar.google.ca/scholar?q=program+synthesis+%28PROSE%29+data+transformations&amp;amp;hl=en&amp;amp;as_sdt=0%2C5&amp;amp;as_vis=1&amp;amp;as_ylo=2015&amp;amp;as_yhi=&#34;&gt;active area of research&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;It remains to be seen whether these tools will become flexible enough for practitioners to use.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Data Science in the Organization: Things I Learned in 2016</title>
      <link>https://alternative-stats.netlify.com/post/2015-07-23-r-rmarkdown.html</link>
      <pubDate>Fri, 30 Dec 2016 21:13:14 -0500</pubDate>
      
      <guid>https://alternative-stats.netlify.com/post/2015-07-23-r-rmarkdown.html</guid>
      <description>&lt;div id=&#34;user-2016&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;useR! 2016&lt;/h1&gt;
&lt;p&gt;&lt;img src=&#34;stanford.jpg&#34; /&gt; The &lt;a href=&#34;http://user2016.org/&#34;&gt;useR! conference&lt;/a&gt; in Stanford was awesome. Never did I expect to see so many participants (read: data geeks) and high level &lt;a href=&#34;http://user2016.org/#sponsors&#34;&gt;sponsors&lt;/a&gt;. For what was a little known language that’s been around in one form or another since the late seventies, this is quite amazing. Some memorable talks:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;People are doing really amazing things with &lt;strong&gt;Shiny&lt;/strong&gt;. I was amazed at the pervasiveness of Shiny at &lt;a href=&#34;http://channel9.msdn.com/Events/useR-international-R-User-conference/useR2016/RServer-Operationalizing-R-at-Electronic-Arts&#34;&gt;EA Games&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;R’s superhero &lt;strong&gt;Hadley Whickham&lt;/strong&gt;, never takes a break. His idea is to create a “&lt;a href=&#34;http://channel9.msdn.com/Events/useR-international-R-User-conference/useR2016/Towards-a-grammar-of-interactive-graphics&#34;&gt;pit of success&lt;/a&gt;” by having a uniform approach to import, tidy, transform, visualize, model and communicate.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Daniela Witten methodology on &lt;strong&gt;&lt;a href=&#34;http://channel9.msdn.com/Events/useR-international-R-User-conference/useR2016/Flexible-and-Interpretable-Regression-Using-Convex-Penalties&#34;&gt;interpretable regression using convex penalties&lt;/a&gt;&lt;/strong&gt;: as I see it, it’s an attempt to do the least damage when people want simple interpretation of non-linear coefficients.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Tal Galili gave a great presentation on &lt;strong&gt;&lt;a href=&#34;http://channel9.msdn.com/Events/useR-international-R-User-conference/useR2016/Heatmaps-in-R-Overview-and-best-practices&#34;&gt;heatmaps&lt;/a&gt;&lt;/strong&gt; in R&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Rick Beckett on &lt;strong&gt;&lt;a href=&#34;http://channel9.msdn.com/Events/useR-international-R-User-conference/useR2016/Forty-years-of-S&#34;&gt;40 years of S&lt;/a&gt;&lt;/strong&gt; was fascinating: now I understand why the assignment operator in R is &lt;code&gt;&amp;lt;-&lt;/code&gt; (hint: jump to 0:29:00). We also learned that S and UNIX were two Bell Labs projects that grew together. Ironic how both took a while to be picked up and both grew into massively popular platforms (Linux, R).&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;data-driven-decisions&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Data-Driven Decisions&lt;/h1&gt;
&lt;p&gt;As a long-time data practitioner, I don’t get the cult-like infatuation with the term “data-driven” decisions (see here, here or here). Really folks: in the best case scenario we go from: data to evidence to decisions. If we want to go from data to decisions, hope for the best, because science may not back you up.&lt;/p&gt;
&lt;p&gt;In health research, the term “data driven” has negative connotations. Instead, we have a much more scientifically sound **[evidence-based]*(&lt;a href=&#34;http://en.wikipedia.org/wiki/Evidence-based_medicine)*&#34; class=&#34;uri&#34;&gt;http://en.wikipedia.org/wiki/Evidence-based_medicine)*&lt;/a&gt; approach.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;evidence_based.png&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;A recent study suggests that &lt;a href=&#34;http://jamanetwork.com/journals/jamainternalmedicine/article-abstract/2593252&#34;&gt;women make better doctors than men&lt;/a&gt; (also &lt;a href=&#34;http://www.theatlantic.com/health/archive/2016/12/female-doctors-superiority/511034/&#34;&gt;here&lt;/a&gt; and &lt;a href=&#34;http://www.nbcnews.com/health/health-news/female-doctors-outperform-male-doctors-according-study-n697876&#34;&gt;here&lt;/a&gt;). There are &lt;a href=&#34;http://stream.org/whos-better-playing-doctor-boys-girls/&#34;&gt;many problems&lt;/a&gt; with this study. But it got me thinking of a way to explain how “data driven” can be dangerous: if women are better doctor because they treated many fewer patients than men and, therefore, had more time to spend with patients, then, the data-driven decision of having more women doctors won’t lead to improved health outcomes. But, hey, it’s “data driven”!&lt;/p&gt;
&lt;p&gt;This echos well-known statistician &lt;a href=&#34;http://vkc.mc.vanderbilt.edu/people/harrell-frank&#34;&gt;Frank Harrell&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Using the data to guide the data analysis is almost as dangerous as not doing so“.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ethics-ethics-ethics&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Ethics, Ethics, Ethics&lt;/h1&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;ethics.jpg&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;The ties between &lt;strong&gt;statistics and ethics&lt;/strong&gt; are well known (see &lt;a href=&#34;http://www.jstor.org/stable/3564481?seq=1#page_scan_tab_contents&#34;&gt;here&lt;/a&gt;). Nearly every PhD biostatistician I know has sat on an ethics board at some points in their careers. Now, it seems that the broader field of &lt;strong&gt;Data Science is starting to pay attention to the ethical implications of their work&lt;/strong&gt;. A few key events:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A great article on ProPublica on &lt;a href=&#34;http://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing&#34;&gt;predictive sentencing&lt;/a&gt; in the US.&lt;/li&gt;
&lt;li&gt;More and more &lt;a href=&#34;http://www.edx.org/course/data-science-ethics-michiganx-ds101x-1&#34;&gt;courses&lt;/a&gt; on ethics in data science are starting to emerge&lt;/li&gt;
&lt;li&gt;Cathy O’Neal’s book “&lt;a href=&#34;http://blogs.scientificamerican.com/roots-of-unity/review-weapons-of-math-destruction/&#34;&gt;Weapon of Math Destruction: How Big Data Increases Inequality and Threatens Democracy&lt;/a&gt;” is on my &lt;a href=&#34;http://a.co/0weW8WA&#34;&gt;Amazon wish list&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Google provides one of the &lt;a href=&#34;http://research.google.com/bigpicture/attacking-discrimination-in-ml/&#34;&gt;best explanations&lt;/a&gt; I have seen on the difficulty of making fair decisions. As &lt;a href=&#34;http://www.statschat.org.nz/2017/01/01/kinds-of-fairness-worth-working-for/&#34;&gt;Thomas Lumley&lt;/a&gt; writes: “&lt;strong&gt;You have to decide what summary of the difference you care about, because you can’t make them all the same&lt;/strong&gt;. This is old news in medical diagnostics, but appears not to have been considered in some other areas”.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;unicorns&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Unicorns&lt;/h1&gt;
&lt;p&gt;&lt;img src=&#34;https://s-media-cache-ak0.pinimg.com/736x/9d/0e/e6/9d0ee6c0049e088172590e0d81b1684c--polygon-art-diy-paper.jpg&#34; /&gt; This year has confirmed that Data Science is a &lt;strong&gt;wide and broad field&lt;/strong&gt;, just like Medicine, Law or Engineering: many flavours, many specializations. Sorry folks, &lt;strong&gt;unicorns who are specialist in everything do not exist&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Just like hospitals do not hire a family physicians and expect them to do the work of 100 specialists, just like we don’t hire a criminal lawyer when we need a tax lawyer, organizations cannot expect to hire one data scientist generalist and do the work of 100 specialists. &lt;a href=&#34;http://drewconway.com/zia/2013/7/18/warning-do-not-feed-the-wildebeests&#34;&gt;&lt;strong&gt;Diversity&lt;/strong&gt;&lt;/a&gt; is key.&lt;/p&gt;
&lt;p&gt;And let’s not fall into the trap of judging a Data Scientist by the &lt;strong&gt;mathematical or algorithmic sophistication&lt;/strong&gt; of their arsenals. In the words of &lt;a href=&#34;http://en.wikipedia.org/wiki/Daniel_Kahneman&#34;&gt;Kahneman&lt;/a&gt;, we would be substituting a question which we cannot answer with a different one which we can answer.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;automation-model-factories&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Automation &amp;amp; Model Factories&lt;/h1&gt;
&lt;p&gt;&lt;img src=&#34;automation.png&#34; /&gt; Automation is really misunderstood. Really. I was pointed to &lt;strong&gt;Tom Davenport’s&lt;/strong&gt; article on “&lt;a href=&#34;http://www.linkedin.com/pulse/move-your-analytics-operation-from-artisanal-tom-davenport?trk=hp-feed-article-title-like&#34;&gt;Autonomous Analytics&lt;/a&gt;” and the concept of &lt;strong&gt;model factory&lt;/strong&gt; as described in &lt;a href=&#34;http://www.youtube.com/watch?v=yNfsnv9gjrU&#34;&gt;this video&lt;/a&gt;. It’s a great article and agree with pretty much everything. It’s what it &lt;strong&gt;does not say&lt;/strong&gt; that concerns me. I commented in Tom’s article, but briefly:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Not everything is worth automating (e.g. single points decisions)&lt;/li&gt;
&lt;li&gt;Not everything is automatable (causal models, explanatory models, exploratory analysis)&lt;/li&gt;
&lt;li&gt;Automation is more of an IT problem than a Data Science one (see &lt;strong&gt;Diego Kuonen’s&lt;/strong&gt; fantastic slides on &lt;a href=&#34;http://www.slideshare.net/kuonen/demystifying-big-data-data-science-and-statistics-along-with-machine-intelligence-and-learning&#34;&gt;Demystifying Big Data, Data Science and Statistics&lt;/a&gt;). &lt;em&gt;Which means it is the last mile of a repetitive insight generation process the mechanisms of which are well understood and that applies to a well understood class of problems (predictive)&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;data-wrangling&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Data Wrangling&lt;/h1&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;data_wrangling.jpg&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;&lt;a href=&#34;http://www.nytimes.com/2014/08/18/technology/for-big-data-scientists-hurdle-to-insights-is-janitor-work.html?_r=0&#34;&gt;Data munging/carpentry/wrangling&lt;/a&gt;, you know, that tedious, time consuming work that &lt;strong&gt;takes some 80% of a data scientist’s time&lt;/strong&gt; is (still) not given enough credit by evangelists, vendors, consultants and speakers. Not sexy? Sure, but it’s a reality we must face nonetheless.&lt;/p&gt;
&lt;p&gt;What’s more, is hardly automatable (see above) and the &lt;a href=&#34;http://www.oreilly.com/ideas/why-data-preparation-frameworks-rely-on-human-in-the-loop-systems&#34;&gt;human-in-the-loop&lt;/a&gt; provides myriads of necessary context and judgement calls that machines cannot make. I bet this has caused a lot of misunderstanding in organizations: “&lt;em&gt;What? Are you trying to tell me we spent $100K for that fancy analytics software and it’s useless?&lt;/em&gt;”&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;analytical-maturity&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Analytical Maturity&lt;/h1&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;http://xolotl.org/wp-content/uploads/2016/11/7159036564_ec69766fe7_o.png&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;If the keywords and verbiage used by organizations to advertise their data related jobs are any indication of their &lt;strong&gt;analytical maturity&lt;/strong&gt;, there are promising signs! I work in the energy sector, not Silicon Valley, which means it takes a little while for us to pick up on technological or scientific innovations. For the past four years, I received weekly job alerts and, anecdotally, I can say that the expectations, clarity and keywords of the ad have improved significantly.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;analytics-podcasts&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Analytics Podcasts&lt;/h1&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://media.licdn.com/mpr/mpr/AAEAAQAAAAAAAAh-AAAAJGMxOWY3NzViLWEwYmItNDBkOS1iNDc3LTkyYTA5OGFmYjA4ZQ.png&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;The number of really good analytics podcasts has grown significantly, to the point I can barely keep up.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://soundcloud.com/nssd-podcast&#34;&gt;Not So Standard Deviation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://dataskeptic.com/&#34;&gt;The Data Skeptic Podcast&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://partiallyderivative.com/&#34;&gt;Partially Derivative&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://datastori.es/&#34;&gt;Data Stories&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.oreilly.com/topics/oreilly-data-show-podcast&#34;&gt;O’Reilly Data Show&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.thetalkingmachines.com/&#34;&gt;Talking Machines&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Not So Standard Deviations and O’Reilly Data Show are great detox if you are subject to a considerable amount of timeshare-sque-like products and hyperbolic claims by vendors or consultants.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
