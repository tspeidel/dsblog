<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Data Analysis on Alternative Stats</title>
    <link>https://alternative-stats.netlify.com/categories/data-analysis.html</link>
    <description>Recent content in Data Analysis on Alternative Stats</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2017 Thomas Speidel</copyright>
    <lastBuildDate>Sat, 30 Dec 2017 00:00:00 +0000</lastBuildDate>
    <atom:link href="/categories/data-analysis.html" rel="self" type="application/rss+xml" />
    
    <item>
      <title>What You Aren&#39;t Told About Data Science</title>
      <link>https://alternative-stats.netlify.com/post/what-you-aren-t-told-about-data-science.html</link>
      <pubDate>Sat, 30 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>https://alternative-stats.netlify.com/post/what-you-aren-t-told-about-data-science.html</guid>
      <description>&lt;p&gt;&lt;img src=&#34;scrubbing.jpg&#34; height=&#34;200px&#34; width=&#34;700px&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Some time ago I started writing a post on data preparation which I never completed and eventually forgot. A recent LinkedIn post by Kevin Gray stimulated a rich conversation around: “&lt;a href=&#34;https://www.linkedin.com/feed/update/urn:li:activity:6352086326745096192&#34;&gt;Can Data Cleaning be automated?&lt;/a&gt;”. It reminded and enticed me to complete the post.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;div id=&#34;data-cleaning-and-data-preparation&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Data Cleaning and Data Preparation&lt;/h1&gt;
&lt;p&gt;When practitioners talk about data cleaning, they usually refer to a collection of tasks needed to &lt;strong&gt;make the data amenable for analysis&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Data is not necessarily invalid when we say they need to be cleaned. A common term in statistics that hasn’t fully caught-on in other areas is that of &lt;strong&gt;convenience data&lt;/strong&gt;: data that has been collected for reasons other than the purpose at task. Convenience data includes most of today’s “Big Data” and certainly most data collected for administrative purposes. Convenience data almost always require varying amounts of preparation because &lt;strong&gt;key variables for the task at hand may be missing&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;In 2014 on the height of the “Big-Data” hype, &lt;a href=&#34;https://www.nytimes.com/2014/08/18/technology/for-big-data-scientists-hurdle-to-insights-is-janitor-work.html&#34;&gt;Steve Lohr&lt;/a&gt; from the New York Times highlighted what practitioners have known for long time, but that came as a surprise to many. Lohr writes:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Working data scientists spend most of their time preparing data for analysis, a process that includes data collection, assessment, and transformation. Building an analysis data set is the first “hands-on” step in predictive analytics; analysts understand that this task is essntial for effective model building, and they invest as much time as needed to get it right.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Lohr goes on to highlight software tools capable of “finding, cleaning and blending data so that it is ready to be analyzed”. Most practitioners are highly skeptical of such claims; and for good reasons: they fail to recognize the complexity of the task. Unsurprisingly, it appears none of these software tools have caught up with data scientists. But why?&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;judgment-and-foresight&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Judgment and Foresight&lt;/h1&gt;
&lt;p&gt;Kuhn and Johnson shed some light in their popular book “&lt;a href=&#34;https://www.amazon.com/Applied-Predictive-Modeling-Max-Kuhn/dp/1461468485&#34;&gt;Applied Predictive Modeling (2013)&lt;/a&gt;”. They write:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;One of the first steps in the model building process is to transform, or encode, the original data structure into a form that is most informative for the model. This encoding process is critical and must be done with &lt;strong&gt;foresight into the analysis&lt;/strong&gt; that will be performed so that appropriate predictors can be elucidated from the original data.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;“&lt;strong&gt;Foresight into the analysis&lt;/strong&gt;” is the key sentence. Practitioners typically have a reasonable idea of what they are trying to achieve and what they do with the data is a reflection of that purpose. Let me illustrate how that “foresight” affects the choices we make with the data:&lt;/p&gt;
&lt;table class=&#34;table table-striped table-hover table-condensed&#34; style=&#34;float: left; margin-right: 10px;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Customer ID
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Purchase
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Item
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2015-06-15
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
A
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2013-08-25
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
C
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2013-08-26
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
A
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2013-08-05
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
D
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class=&#34;table table-striped table-hover table-condensed&#34; style=&#34;float: left; margin-right: 10px;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Customer ID
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Purchase Month
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
A Items
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
B Items
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
C Items
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
D Items
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
June
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
August
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
August
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class=&#34;table table-striped table-hover table-condensed&#34; style=&#34;float: left; margin-right: 10px;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Customer ID
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Purchase
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Item
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2015-06-15
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
A
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2015-06-16
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2015-06-17
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
…
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
…
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2017-12-30
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2013-08-25
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
C
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2013-08-26
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
A
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2013-08-27
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
…
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
…
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2017-12-30
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2013-08-05
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
D
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2013-08-06
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
…
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
…
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2017-12-30
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;All three data-sets are realizations of the same underlying data, a fictitious customer purchase record. Neither is right or wrong, yet they all represent different “foresight” into how we attempt to analyze the data. There could be hundreds of ways to represent these data and each one of them may address a different analytically objective.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;can-data-be-clean&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Can Data Be Clean?&lt;/h1&gt;
&lt;p&gt;So, what’s clean data? Notwithstanding data entry errors, it may not exist writes &lt;a href=&#34;http://www.mimno.org/articles/carpentry/&#34;&gt;David Mimno (2014)&lt;/a&gt; from Cornell University:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;To me these imply that there is some kind of pure or clean data buried in a thin layer of non-clean data, and that one need only hose the dataset of to reveal the hard porcelain underneath the muck. In reality, the process is more like deciding how to cut into a piece of material, or how much to plane down a surface. It’s not that there’s any real distinction between good and bad, it’s more that some parts are or knottier than others. &lt;strong&gt;Judgement is critical&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;Foresight and now judgement&lt;/strong&gt;: two very human traits, both hard to automate except in very narrow problems.&lt;/p&gt;
&lt;p&gt;We must also remind ourselves that clean data may not be achievable in the first place. With respect to data with errors, Bill Winkler provides plenty of examples on the complexities of &lt;a href=&#34;https://en.wikipedia.org/wiki/Record_linkage&#34;&gt;record linkage&lt;/a&gt; where exact matches may not be feasible to achieve (see &lt;a href=&#34;http://onlinelibrary.wiley.com/doi/10.1002/wics.1317/abstract&#34;&gt;Winkler, 2014&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.amazon.com/Advanced-Analytics-Methodologies-Driving-Business/dp/0133498603&#34;&gt;Chambers and Dinsmore (2014)&lt;/a&gt; write on the importance of this time consuming steps:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Working data scientists spend most of their time preparing data for analytical a process that includes data collection, assessment, and transformation. Building an analysis data set is the first (hands-on&amp;quot; step in predictive analytics; analysts understand that this task is &lt;strong&gt;essential for effective model building&lt;/strong&gt;, and they invest as much time as needed to get it right.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;why-the-surprise&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Why the Surprise?&lt;/h1&gt;
&lt;p&gt;For practitioners, it is hardly a surprise data preparation is tedious and time consuming requiring foresight and judgement. Why the surprise then? &lt;a href=&#34;https://cs.uwaterloo.ca/~ilyas/&#34;&gt;Ihab Ilyas&lt;/a&gt; a professor at the University of Waterloo explains In a podcast interview on &lt;a href=&#34;https://www.oreilly.com/ideas/why-data-preparation-frameworks-rely-on-human-in-the-loop-systems&#34;&gt;O’Reilly Data Show&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;It has been also difficult to communicate these results to industry. And database practitioners, if you like, they were more into the well structured data and assuming a lot of good properties around this data, [and they were also] more interested in indexing this data, storing it, moving it from one place to another. And now, dealing with this large amount of diverse heterogeneous data with tons of errors, sidled across all business units in the same enterprise became a necessity. You cannot really avoid that anymore.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Perhaps the problem lies in part with analytically literacy: organizations have traditionally dealt with straight forward analytically needs for which the data at hand was sufficiently usable: &lt;strong&gt;filtering, joining, aggregating, adding, sorting&lt;/strong&gt; etc. For instance: &lt;em&gt;how many widgets were sold in store 11 in Q1 of 2016?&lt;/em&gt; In a typical organization, many inconsistencies of data were carried forward, at times unnoticed, and left to the consumer of the data to deal with. This paradigm no longer holds in the era of data as an asset where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the inconsistencies in the data may preclude more sophisticated analysis and certainly inferential and predictive objectives&lt;/li&gt;
&lt;li&gt;the data scientist both prepares and analyzes the data&lt;/li&gt;
&lt;li&gt;the data scientist expects to work with raw, unprocessed data&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Both Ilyas and Mimno highlight one aspect of doing data analysis data scientists may struggle with: &lt;strong&gt;single source of the truth&lt;/strong&gt;. This concept helps IT design robust data warehouse systems, but is a misleading principle to adhere to when doing analysis.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;can-data-preparation-be-automated&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Can Data Preparation Be Automated?&lt;/h1&gt;
&lt;p&gt;In my view, the short answer is no. Attempts to automate this tedious and time consuming task have largely failed because we do not know how to address the underlying problem. This does not mean that attempts at &lt;em&gt;facilitating&lt;/em&gt; data preparation tasks have failed. On the contrary, there has been progress in this area. In a &lt;a href=&#34;file:///home/tspeidel/GoogleDrive/dsblog/GitHub/dsblog/content/post/2017-12-30-what-you-aren-t-told-about-data-science.html&#34;&gt;recent Data Skeptic podcast&lt;/a&gt;, Microsoft’s Joseph Sirosh reports that:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Building on the latest research in program synthesis (PROSE) and data cleaning, Microsoft created a data wrangling technology that can drastically reduce the time that data scientists have to spend in coding and transforming data for machine learning. The way program synthesis works are, you give an input, the kind of data you want to wrangle, and the output you want to transform to, so you control the before and after.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I have not looked into these tools. Nonetheless, this seems significantly more tractable than auto-magically “&lt;em&gt;finding, cleaning and blending data so that it is ready to be analyzed&lt;/em&gt;”. A search in the academic literature reveals this to be an &lt;a href=&#34;https://scholar.google.ca/scholar?q=program+synthesis+%28PROSE%29+data+transformations&amp;amp;hl=en&amp;amp;as_sdt=0%2C5&amp;amp;as_vis=1&amp;amp;as_ylo=2015&amp;amp;as_yhi=&#34;&gt;active area of research&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;It remains to be seen whether these tools will become flexible enough for practitioners to use.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>One Common Mistake Data Analysts Make and How to Avoid It</title>
      <link>https://alternative-stats.netlify.com/post/one-common-mistake-data-analysts-make-and-how-to-avoid-it.html</link>
      <pubDate>Tue, 19 Jan 2016 00:00:00 +0000</pubDate>
      
      <guid>https://alternative-stats.netlify.com/post/one-common-mistake-data-analysts-make-and-how-to-avoid-it.html</guid>
      <description>&lt;p&gt;One common mistake inexperienced analyst do in analysing data is that of &lt;strong&gt;assigning a special role to time&lt;/strong&gt;. In my line of work, I see this a lot. Signs of &lt;em&gt;chronophrenia&lt;/em&gt;, a term I just made up, are the unusual focus on calendar time: from line plots of multiple variables on the y-axis against time on the x-axis, to frequencies or averages by some meaningful time cut-off: monthly, quarterly, yearly and so on.&lt;/p&gt;
&lt;p&gt;Don’t get me wrong, I think there are plenty of scenarios when looking at time makes perfect sense and both the statistical, engineering and econometric literature continue to make substantial contributions to this area (I can think of &lt;a href=&#34;https://en.wikipedia.org/wiki/Change_detection&#34;&gt;change point detection&lt;/a&gt;, &lt;a href=&#34;https://en.wikipedia.org/wiki/Kalman_filter&#34;&gt;Kalman filters&lt;/a&gt; and &lt;a href=&#34;http://www.springer.com/cda/content/document/cda_downloaddocument/9780387772370-c1.pdf?SGWID=0-0-45-771009-p173891512&#34;&gt;Bayesian/Dynamic time series&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;Often the analyst attempts to visually assess whether a &lt;strong&gt;shift in time exists&lt;/strong&gt;. The hope is that the shift can be tied to a special cause known to the analyst (for instance a change that was introduced at known point in time) and that this would represent evidence in favour or against a hypothesis.&lt;/p&gt;
&lt;p&gt;There are a number of drawbacks with this approach. Here I will list a few common ones:&lt;/p&gt;
&lt;p&gt;How do we know whether a change in time is due to some special cause that happens at a know time, or whether is due to normal variation? In these settings SPC is limited.&lt;/p&gt;
&lt;p&gt;Time does not possess magical properties: in most fields, seldom is time on the causal pathway. In other words, time does not cause much. Ok, there are some exceptions, however, at best, time is a proxy measure for something we are unable to measure. Think about it: how many times can we think of time having caused something? Death? Not really: trauma, or the inability of cells to reproduce reliably are some of the underlying causes. An old engine breaking down? Think fatigue, structural failure, wear and tear. Changing jobs? Think of complex social issues.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;Once you do fully understand a process, time plays no role (Cleves et al.)&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Suppose we introduce some change at a known point in time. The analyst proceeds to compare, often visually, whether a slope or a shift change occurs. The approach is limited unless we can somehow freeze everything else.&lt;/p&gt;
&lt;p&gt;To an untrained analyst, time is an open invitation to slice and dice the data until some interesting results are found: we will always find a reason for looking at things daily, weekly, monthly, quarterly, yearly. These are in fact, meaningful measures to a lot of businesses. Eventually, you are guaranteed to find something interesting or even significant. But it does not mean it’s true. But wait! Don’t take my word for it! &lt;a href=&#34;http://www.tylervigen.com/spurious-correlations&#34;&gt;Tyler Vigen&lt;/a&gt; assembled a very humorous collection of &lt;strong&gt;spurious correlations&lt;/strong&gt;. Vigen had enough material to fill a whole book. You don’t have to buy his book, though I would encourage you to do so. Some are available on Vigen’s website.&lt;/p&gt;
&lt;p&gt;Visualization pioneer Edward Tufte has a very effective visual demonstration on &lt;strong&gt;streak-guessing&lt;/strong&gt; and &lt;strong&gt;over-narrative&lt;/strong&gt; around time (here, adapted from the &lt;a href=&#34;https://www.edwardtufte.com/bboard/q-and-a-fetch-msg?msg_id=0001OR&#34;&gt;original&lt;/a&gt;). On the top figure we see the actual win-loss record. At the bottom, notice what happens when we randomize the order on time.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;winning_streak_1.png&#34; /&gt; &lt;em&gt;2009 Boston Red Sox win-loss record: when 4 or more wins occurs one after the other, the series is drawn in red. The causal attribution of win or loss streaks result in over-narratives.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;winning_streak_2.png&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;&lt;em&gt;Not so fast: 3 randomized samples from the same data. There is little evidence for a causal mechanism of win or loss streaks that in the original series resulted in over-narratives. The invisible hand of chance.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;So, enough about ranting against time. Let me play devil’s advocate and list some arguments in favour of time:&lt;/p&gt;
&lt;p&gt;When we do not understand a process, time is often a good proxy for something we are unable to measure. We should try to smooth time to detect trends. I’m a fan of LOESS for its flexibility.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;loess.gif&#34; alt=&#34;LOESS&#34; /&gt; LOESS (image from &lt;a href=&#34;https://simplystatistics.org/2017/08/08/code-for-my-educational-gifs/&#34;&gt;Simply Statistics&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;Survival analysis (time to event analysis) is mostly concerned with the rate at which things are moving. Is a certain group reaching an event faster than another group? Survival analysis can effectively deal with survivorship bias.&lt;/p&gt;
&lt;p&gt;When we introduce a change at a given time point, there are methods that try to deal with it, such as interrupted time series or &lt;a href=&#34;https://en.wikipedia.org/wiki/Change_detection&#34;&gt;change point detection&lt;/a&gt; (these can go by different names). For an overview, see Kontopantelis. Change point detection is an active area of research.&lt;/p&gt;
&lt;p&gt;If you are embarking on an analysis, don’t jump on time as your first go-to measure. Think carefully about the problem and try first to identify all factors that may affect a response of interest. Explore those first, plot them against the response, plot them against each other. Try to learn as much about your problem without recurring to time immediately. When you do look at time, remember there are challenges unique to times that complicates things (autocorrelation and censoring to name a couple).&lt;/p&gt;
&lt;p&gt;Remember: “&lt;em&gt;Once you do fully understand a process, time plays no role&lt;/em&gt;” … or almost.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
